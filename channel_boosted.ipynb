{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sugar', 'cup', 'sweet', 'ac', 'bill', 'thank-you', 'water', 'no', 'vegetables', 'pop corn', 'bitter', 'milk', 'allergy', 'spicy', 'cost', 'ice cream', 'chair', 'cheese', 'napkin', 'about', 'small', 'french fries', 'manager', 'salt', 'cold', 'warm', 'ingredients', 'burger', 'alcohol', 'eggs', 'chicken', 'what', 'bread', 'hello', 'sauce', 'bag', 'pizza', 'pepper', 'drink', 'which', 'gluten free']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "dataset_path = os.listdir('/Users/vedantpadole/Desktop/Research/ASL_MY_DATASET')\n",
    "\n",
    "all_classes = os.listdir('/Users/vedantpadole/Desktop/Research/ASL_MY_DATASET')\n",
    "print (all_classes)  \n",
    "len(all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT,IMAGE_WIDTH=128,128\n",
    "SEQUENCE_LENGTH=10\n",
    "DATASET='/Users/vedantpadole/Desktop/Research/ASL_MY_DATASET'\n",
    "CLASSES_LIST=all_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_extraction(video_path):\n",
    "  frames_list=[]\n",
    "  video_reader=cv2.VideoCapture(video_path)\n",
    "  video_frames_count=int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "  skip_frames_window=max(int(video_frames_count/SEQUENCE_LENGTH),1)\n",
    "  for frame_counter in range(SEQUENCE_LENGTH):\n",
    "    video_reader.set(cv2.CAP_PROP_POS_FRAMES,frame_counter*skip_frames_window)\n",
    "    success,frame=video_reader.read()\n",
    "    if not success:\n",
    "      break\n",
    "    resized_frame=cv2.resize(frame,(IMAGE_HEIGHT,IMAGE_WIDTH))\n",
    "    normalized_frame=resized_frame/255\n",
    "    frames_list.append(normalized_frame)\n",
    "  video_reader.release()\n",
    "  return frames_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "  features=[]\n",
    "  labels=[]\n",
    "  video_files_path=[]\n",
    "  for class_index,class_name in enumerate(CLASSES_LIST):\n",
    "    print(f'Extracting the data of class:  {class_name}')\n",
    "    files_list=os.listdir(os.path.join(DATASET,class_name))\n",
    "    for file_name in files_list:\n",
    "      video_file_path=os.path.join(DATASET,class_name,file_name)\n",
    "      frames=frames_extraction(video_file_path)\n",
    "      if len(frames)==SEQUENCE_LENGTH:\n",
    "        features.append(frames)\n",
    "        labels.append(class_index)\n",
    "        video_files_path.append(video_file_path)\n",
    "  features=np.asarray(features)\n",
    "  labels=np.array(labels)\n",
    "  return features,labels,video_files_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the data of class:  sugar\n",
      "Extracting the data of class:  cup\n",
      "Extracting the data of class:  sweet\n",
      "Extracting the data of class:  ac\n",
      "Extracting the data of class:  bill\n",
      "Extracting the data of class:  thank-you\n",
      "Extracting the data of class:  water\n",
      "Extracting the data of class:  no\n",
      "Extracting the data of class:  vegetables\n",
      "Extracting the data of class:  pop corn\n",
      "Extracting the data of class:  bitter\n",
      "Extracting the data of class:  milk\n",
      "Extracting the data of class:  allergy\n",
      "Extracting the data of class:  spicy\n",
      "Extracting the data of class:  cost\n",
      "Extracting the data of class:  ice cream\n",
      "Extracting the data of class:  chair\n",
      "Extracting the data of class:  cheese\n",
      "Extracting the data of class:  napkin\n",
      "Extracting the data of class:  about\n",
      "Extracting the data of class:  small\n",
      "Extracting the data of class:  french fries\n",
      "Extracting the data of class:  manager\n",
      "Extracting the data of class:  salt\n",
      "Extracting the data of class:  cold\n",
      "Extracting the data of class:  warm\n",
      "Extracting the data of class:  ingredients\n",
      "Extracting the data of class:  burger\n",
      "Extracting the data of class:  alcohol\n",
      "Extracting the data of class:  eggs\n",
      "Extracting the data of class:  chicken\n",
      "Extracting the data of class:  what\n",
      "Extracting the data of class:  bread\n",
      "Extracting the data of class:  hello\n",
      "Extracting the data of class:  sauce\n",
      "Extracting the data of class:  bag\n",
      "Extracting the data of class:  pizza\n",
      "Extracting the data of class:  pepper\n",
      "Extracting the data of class:  drink\n",
      "Extracting the data of class:  which\n",
      "Extracting the data of class:  gluten free\n"
     ]
    }
   ],
   "source": [
    "features,labels,video_files_path=create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "one_hot_encoded_labels=to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_constant=27\n",
    "np.random.seed(seed_constant)\n",
    "random.seed(seed_constant)\n",
    "tf.random.set_seed(seed_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "features_train,features_test,labels_train,labels_test=train_test_split(features,one_hot_encoded_labels,test_size=0.2,shuffle=True,random_state=seed_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=features_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Reshape, Multiply, Bidirectional, LSTM, Dense\n",
    "\n",
    "# Assuming you have already defined IMAGE_HEIGHT, IMAGE_WIDTH, and CLASSES_LIST\n",
    "\n",
    "def create_cbam_bilrcn_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # CBAM - Convolutional Block Attention Module\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', data_format='channels_last', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # Add CBAM attention here (you can implement it as a separate function or a custom layer)\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', data_format='channels_last'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # Add CBAM attention here\n",
    "    \n",
    "    # Reshape before BiLRCN\n",
    "    model.add(Reshape((SEQUENCE_LENGTH, -1)))  # Use -1 to flatten the spatial dimensions\n",
    "    model.add(Bidirectional(LSTM(units=32, activation='relu', return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(units=64, activation='relu', return_sequences=True)))\n",
    "    \n",
    "    # Global pooling or TimeDistributed pooling can be used here based on the data and problem\n",
    "    model.add(GlobalAveragePooling2D())  # or TimeDistributed(Dense(64)) + GlobalAveragePooling1D()\n",
    "    \n",
    "    model.add(Dense(len(CLASSES_LIST), activation='softmax'))\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"global_average_pooling2d_23\" is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: (None, 10, 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/vedantpadole/Desktop/Research/channel_boosted.ipynb Cell 12\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/vedantpadole/Desktop/Research/channel_boosted.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model1\u001b[39m=\u001b[39mcreate_cbam_bilrcn_model()\n",
      "\u001b[1;32m/Users/vedantpadole/Desktop/Research/channel_boosted.ipynb Cell 12\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vedantpadole/Desktop/Research/channel_boosted.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m model\u001b[39m.\u001b[39madd(Bidirectional(LSTM(units\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, return_sequences\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vedantpadole/Desktop/Research/channel_boosted.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Global pooling or TimeDistributed pooling can be used here based on the data and problem\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vedantpadole/Desktop/Research/channel_boosted.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m model\u001b[39m.\u001b[39;49madd(GlobalAveragePooling2D())  \u001b[39m# or TimeDistributed(Dense(64)) + GlobalAveragePooling1D()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vedantpadole/Desktop/Research/channel_boosted.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m model\u001b[39m.\u001b[39madd(Dense(\u001b[39mlen\u001b[39m(CLASSES_LIST), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vedantpadole/Desktop/Research/channel_boosted.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m model\u001b[39m.\u001b[39msummary()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/trackable/base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/input_spec.py:235\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    233\u001b[0m     ndim \u001b[39m=\u001b[39m shape\u001b[39m.\u001b[39mrank\n\u001b[1;32m    234\u001b[0m     \u001b[39mif\u001b[39;00m ndim \u001b[39m!=\u001b[39m spec\u001b[39m.\u001b[39mndim:\n\u001b[0;32m--> 235\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    237\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mis incompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected ndim=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m, found ndim=\u001b[39m\u001b[39m{\u001b[39;00mndim\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    239\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFull shape received: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(shape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m         )\n\u001b[1;32m    241\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mmax_ndim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     ndim \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"global_average_pooling2d_23\" is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: (None, 10, 128)"
     ]
    }
   ],
   "source": [
    "model1=create_cbam_bilrcn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping_call=EarlyStopping(monitor='val_loss',patience=10,mode='min',restore_best_weights=True)\n",
    "model1.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "r=model1.fit(x=features_train,y=labels_train,epochs=50,batch_size=32,shuffle=True,validation_split=0.2,callbacks=[early_stopping_call])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
